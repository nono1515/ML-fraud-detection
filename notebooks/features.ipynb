{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features analysis\n",
    "\n",
    "In this notebook, features are analysed through different methods in order to select\n",
    "and keep the most important one to reduce the feature space. This may reduce overfitting\n",
    "and improve the model performance.\n",
    "\n",
    "The selected features will then be filtered in the `prepare` stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shap\n",
    "import xgboost\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use only trained data and split them into train and validation datasets. We keep the\n",
    "test set for final evaluation of the performance only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/split/train.csv\")\n",
    "\n",
    "y = train[\"Class\"]\n",
    "X = train.drop([\"Class\"], axis=1)\n",
    "\n",
    "X.shape, y.shape, train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must first defined the parameters of the model we want to use. Those can be found \n",
    "with `scripts/grid_search.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"device\": \"gpu\",\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": 0.5,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"eval_metric\": \"aucpr\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(X.keys())\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "model = XGBClassifier(**params).fit(X_train, y_train)\n",
    "model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by using the default importance from the xgboost librairy. It's not really\n",
    "consistent, but by keeping the top-3 of each, we get the following:\n",
    "- V4\n",
    "- V14\n",
    "- V15\n",
    "- V7\n",
    "- V17\n",
    "- V16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for importance_type in (\"weight\", \"gain\", \"cover\"):\n",
    "    xgboost.plot_importance(model, importance_type=importance_type)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Scikit-learn` permutation importance (more information [here](https://scikit-learn.org/stable/modules/permutation_importance.html)), \n",
    "we can determine how much a feature degrades the model performance if it is swapped with another random one for random samples.\n",
    "\n",
    "We can already see some differences with the basic feature importance from XGBoost. In\n",
    "the top-5, only two features are in common with the one selected previousy:\n",
    "- V14\n",
    "- V4 \n",
    "  \n",
    "And the new ones:\n",
    "- V10 \n",
    "- V26\n",
    "- V12\n",
    "\n",
    "We also observe that the top-5 important features are the same for the training and\n",
    "validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = permutation_importance(model, X_val, y_val, n_repeats=10, random_state=42, n_jobs=4)\n",
    "sorted_importances_idx = r.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    r.importances[sorted_importances_idx].T,\n",
    "    columns=X.columns[sorted_importances_idx],\n",
    ")\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = permutation_importance(\n",
    "    model, X_train, y_train, n_repeats=10, random_state=42, n_jobs=1\n",
    ")\n",
    "sorted_importances_idx = r.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    r.importances[sorted_importances_idx].T,\n",
    "    columns=X.columns[sorted_importances_idx],\n",
    ")\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (train set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can try to do it with [SHAP](https://shap.readthedocs.io/en/latest/index.html)\n",
    "which is a dedicated librairy to measure features importance. Their documentation also\n",
    "includes an [example](https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Census%20income%20classification%20with%20XGBoost.html) to work with XGBoost.\n",
    "\n",
    "Most of the top-10 features were already mentionned with the other methods, only 3 are \n",
    "new:\n",
    "- Ammount\n",
    "- V18\n",
    "- V19\n",
    "\n",
    "Overall, thoses methods are quite consistent with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = xgboost.DMatrix(X_train, label=y_train)\n",
    "d_val = xgboost.DMatrix(X_val, label=y_val)\n",
    "\n",
    "model = xgboost.train(\n",
    "    params,\n",
    "    d_train,\n",
    "    5000,\n",
    "    evals=[(d_val, \"test\")],\n",
    "    verbose_eval=100,\n",
    "    early_stopping_rounds=20,\n",
    ")\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "shap.summary_plot(shap_values, X, plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
